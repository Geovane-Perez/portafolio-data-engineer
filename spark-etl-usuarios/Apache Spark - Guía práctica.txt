# ‚úÖ Apache Spark - Gu√≠a pr√°ctica en PySpark (Google Colab)
Este archivo contiene todos los comandos Spark que usamos, explicados paso a paso, con notas √∫tiles sobre qu√© hace cada instrucci√≥n.

---

## 1. üîß Inicializar sesi√≥n Spark
python
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("EjemploSpark").getOrCreate()

*Explicaci√≥n:* Se crea una sesi√≥n de Spark. SparkSession es el punto de entrada principal para trabajar con DataFrames en PySpark.

---

## 2. üì• Leer un archivo CSV
python
df = spark.read.csv("/content/usuarios.csv", header=True, inferSchema=True)

*Explicaci√≥n:* Se lee un archivo CSV y se crea un DataFrame.  
- header=True: Usa la primera l√≠nea como nombres de columnas.  
- inferSchema=True: Detecta autom√°ticamente los tipos de datos.

---

## 3. üëÄ Ver primeras filas
python
df.show()

*Explicaci√≥n:* Muestra las primeras 20 filas del DataFrame. √ötil para inspeccionar los datos.

---

## 4. üìä Ver el esquema del DataFrame
python
df.printSchema()

*Explicaci√≥n:* Muestra las columnas, tipos de datos y si aceptan nulos. Equivalente a DESCRIBE en SQL.

---

## 5. üìà Estad√≠sticas generales
python
df.describe().show()

*Explicaci√≥n:* Devuelve estad√≠sticas b√°sicas como media, desviaci√≥n est√°ndar, etc.

---

## 6. üéØ Seleccionar columnas espec√≠ficas
python
df.select("nombre", "edad").show()

*Explicaci√≥n:* Muestra solo las columnas especificadas.

---

## 7. üéõÔ∏è Filtrar datos
python
from pyspark.sql.functions import col
df.filter(col("edad") > 30).show()

*Explicaci√≥n:* Filtra los registros donde la edad sea mayor a 30.  
Es equivalente a WHERE edad > 30 en SQL.

---

## 8. üßÆ Agregaciones por grupo
python
from pyspark.sql.functions import avg, max, min
df.groupBy("ciudad").agg(
    avg("edad").alias("edad_promedio"),
    max("edad").alias("edad_maxima"),
    min("edad").alias("edad_minima")
).show()

*Explicaci√≥n:* Agrupa por ciudad y calcula agregados. .alias() cambia el nombre de la columna resultado.

---

## 9. ‚ûï Agregar columna condicional
python
from pyspark.sql.functions import when

df = df.withColumn("es_adulto", when(col("edad") >= 18, True).otherwise(False))

*Explicaci√≥n:* Crea la columna es_adulto, que vale True si edad >= 18, y False en caso contrario.

---

## 10. üß† Agregar columna con rangos
python
df = df.withColumn("rango_edad", 
    when(col("edad") < 18, "menor")
    .when((col("edad") >= 18) & (col("edad") < 30), "no es")
    .when((col("edad") >= 30) & (col("edad") < 60), "adulto")
    .otherwise("mayor")
)

*Explicaci√≥n:* Se usa withColumn y when para asignar categor√≠as seg√∫n rangos de edad.

---

## 11. üß™ Filtrar m√∫ltiples condiciones
python
df_filtrado = df.filter((col("es_adulto") == True) & (col("edad") > 40))

*Explicaci√≥n:* Crea un nuevo DataFrame con condiciones m√∫ltiples (AND l√≥gico).

---

## 12. üíæ Guardar DataFrame en CSV
python
df_filtrado.write.mode("overwrite").option("header", True).csv("/content/usuarios_filtrados")

*Explicaci√≥n:* Guarda el DataFrame como CSV.  
- .mode("overwrite"): Reemplaza si ya existe.  
- .option("header", True): Agrega cabecera con nombres de columnas.

---

## üß† GLOSARIO R√ÅPIDO

- df: variable est√°ndar para representar un DataFrame.
- show(): imprime las primeras filas del DataFrame.
- import: se usa para traer funciones necesarias.
- col(): accede a una columna por nombre.
- withColumn(): crea o reemplaza columnas.
- when(): estructura condicional (como if).
- filter(): filtra filas seg√∫n condici√≥n.
- groupBy().agg(): agrupa filas y aplica funciones como avg, max, etc.

---
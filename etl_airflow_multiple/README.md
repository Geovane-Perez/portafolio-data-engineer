ğŸ› ï¸ Proyecto ETL DinÃ¡mico con Airflow y PostgreSQL

Fecha de implementaciÃ³n: Agosto 2025 
Autor: Geovane PÃ©rez
Repositorio: portafolio-data-engineer

ğŸ¯ Objetivo del Proyecto

Construir un pipeline ETL flexible y escalable usando Apache Airflow, capaz de procesar mÃºltiples archivos .csv con base en archivos de configuraciÃ³n .json, validando y cargando los datos en una base de datos PostgreSQL alojada en AWS RDS.

âš™ï¸ Componentes Clave
	â€¢	Airflow DAG (etl_multi_config_dag.py): Ejecuta el pipeline usando un BashOperator.
	â€¢	Script ETL (etl_multi_config.py):
	â€¢	Lee todos los archivos CSV en la carpeta /data/pendientes.
	â€¢	Busca un archivo .json con la configuraciÃ³n correspondiente en /data/config/.
	â€¢	Valida los datos (campos nulos, tipos, rangos).
	â€¢	Inserta datos vÃ¡lidos en la tabla configurada (ej. users).
	â€¢	Registra datos invÃ¡lidos en la tabla users_rechazados.
	â€¢	Mueve el archivo a /data/procesado/ o /data/error/ segÃºn resultado.
	â€¢	Registra el estado de cada archivo en un log CSV (etl_multi_log.csv).

ğŸ§± Estructura de Carpetas:

/dags
  â””â”€â”€ etl_multi_config_dag.py          â† DAG de Airflow

/scripts
  â””â”€â”€ etl_multi_config.py              â† Script ETL dinÃ¡mico

/data
  â”œâ”€â”€ pendientes/                      â† CSVs por procesar
  â”œâ”€â”€ procesado/                       â† CSVs procesados con Ã©xito
  â”œâ”€â”€ error/                           â† CSVs con errores
  â””â”€â”€ config/
      â””â”€â”€ archivo.json                 â† Configuraciones especÃ­ficas por archivo

/logs
  â””â”€â”€ etl_multi_log.csv                â† Log con estado por archivo

Cada archivo CSV debe tener un .json con el mismo nombre (sin la extensiÃ³n) dentro de /data/config/.

ğŸ§ª Validaciones implementadas
	â€¢	Campos requeridos (validar_no_nulos)
	â€¢	Tipos de datos esperados (int, float, str)
	â€¢	Rango de enteros compatible con BIGINT
	â€¢	InserciÃ³n Ãºnica por id (ON CONFLICT DO NOTHING)
	â€¢	Registro de errores en la tabla users_rechazados con detalle del motivo

ğŸ˜ Base de Datos
	â€¢	PostgreSQL en AWS RDS
	â€¢	Tablas:
	â€¢	users (carga principal)
	â€¢	users_rechazados (datos invÃ¡lidos + motivo del rechazo)

ğŸ³ Infraestructura
	â€¢	Apache Airflow 2.8.2 (Docker)
	â€¢	Python 3.x
	â€¢	PostgreSQL en AWS
	â€¢	Docker Compose para levantar servicios (no incluido en repo por seguridad)
	â€¢	.env y archivos sensibles no se suben por buenas prÃ¡cticas

ğŸš€ EjecuciÃ³n del DAG

Este DAG no tiene schedule_interval definido. Se ejecuta bajo demanda desde la UI de Airflow.

python /opt/airflow/scripts/etl_multi_config.py

Esto se ejecuta internamente mediante el BashOperator definido en el DAG.

âœ… Resultados Esperados
	â€¢	Archivos vÃ¡lidos son cargados en PostgreSQL.
	â€¢	Archivos con errores son movidos y logueados.
	â€¢	El proceso es flexible: puedes aÃ±adir mÃ¡s archivos .csv + .json sin modificar el cÃ³digo.
	â€¢	Proyecto listo para escalar hacia nuevos datasets o formatos.




## Fecha de implementación: Julio 2025

## Pipeline ETL - Airflow + PostgreSQL

*Objetivo*: Orquestar un flujo ETL que lea un archivo CSV, transforme los datos y los cargue a una base de datos PostgreSQL en AWS. Luego, mover el archivo procesado a una carpeta de respaldo.

## DAG 1: etl_pipeline
- Ejecuta un script Python (etl.py) que:
  - Lee archivo CSV desde /opt/airflow/data.
  - Aplica transformaciones básicas (ej. limpieza, capitalización).
  - Validación de datos para campo email nullo.
  - Inserta los datos en una tabla PostgreSQL.

## DAG 2: post_process_pipeline
- Se lanza automáticamente al terminar el DAG 1.
- Mueve el archivo procesado de /data a /data/procesado usando shutil.
- Shutil: Es un modulo de Python que sirve para gestionar archivos y carpetas (mover,copiar,eliminar,etc.).En este caso se ocupa en el archivo mover_archivo.py con el comando shutil.move(origen, destino).

## Componentes clave
- PythonOperator para tareas ETL y mover archivos.
- TriggerDagRunOperator para encadenar DAGs.
- Logs y errores gestionados con Airflow UI.

*Ruta consolidada del proyecto*: E:/opt/airflow

---

*Resultado*: El pipeline funciona correctamente, con ejecución automatizada y manejo de archivos post-proceso.

## Herramientas utilizadas:
- Apache Airflow 2.8.2 (Docker)
- Python 3.x
- PostgreSQL (AWS RDS)
- shutil (manejo de archivos)
- Docker Compose

## Estructura del proyecto:
- /dags → Definiciones de DAGs
- /data → Archivos CSV por procesar
- /data/procesado → Archivos ya procesados
- /scripts → Script etl.py
- /logs → Registros de ejecución

## Resultado final:
- DAGs encadenados correctamente.
- Inserción exitosa de datos limpios en PostgreSQL.
- Movimiento del archivo a carpeta /procesado tras ejecución exitosa.
- Proyecto listo para escalar con nuevas fuentes o tareas adicionales.